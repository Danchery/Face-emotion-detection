{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e135633-6834-4a21-9a48-0bb5af8979b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting process_frame.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile process_frame.py\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "def process_frame(frame, detector, classifier_transforms, classifier, classes, device):\n",
    "    \"\"\"\n",
    "    Procces the given frame input with detection and classification models\n",
    "    \"\"\"\n",
    "    #detection\n",
    "    results = detector(frame, verbose=False)\n",
    "    \n",
    "    detections = results[0]\n",
    "\n",
    "    for detection in detections:\n",
    "    \n",
    "        x_center, y_center, width, height  = detection.boxes.xywh.tolist()[0]\n",
    "        start_y = float(y_center - height / 2)\n",
    "        start_x = float(x_center - width / 2)\n",
    "        width = float(width)\n",
    "        height = float(height)\n",
    "        \n",
    "        picture = Image.fromarray(results[0].orig_img)\n",
    "        croped_picture = torchvision.transforms.functional.crop(picture, start_y, start_x, height, width)\n",
    "        \n",
    "        #plt.imshow(croped_picture)\n",
    "        \n",
    "        transformed_picture = classifier_transforms(croped_picture).unsqueeze(0).to(device)\n",
    "        #plt.imshow(torchvision.transforms.functional.to_pil_image(transformed_picture.squeeze(0)))\n",
    "        \n",
    "        #classification\n",
    "        classifier.eval()\n",
    "        with torch.inference_mode():\n",
    "            logits = classifier(transformed_picture.to(device))\n",
    "            pred_prob = torch.softmax(logits, dim=1)\n",
    "            proba = max(pred_prob.tolist()[0])\n",
    "            emotion = classes[torch.argmax(pred_prob, dim=1).item()]\n",
    "    \n",
    "        start_x, start_y, end_x, end_y = detection.boxes.xyxy.tolist()[0]\n",
    "        start_x = int(start_x)\n",
    "        start_y = int(start_y)\n",
    "        end_x = int(end_x)\n",
    "        end_y = int(end_y)\n",
    "    \n",
    "        frame = results[0].orig_img\n",
    "\n",
    "        #draw bb\n",
    "        color = (0, 191, 255)\n",
    "        thickness = 2\n",
    "        cv2.rectangle(img=frame, pt1=(start_x, start_y), pt2=(end_x, end_y), color=color, thickness = thickness)\n",
    "\n",
    "        # put the text on the image\n",
    "        org = (start_x, start_y)# - 20)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fontScale = 1\n",
    "        text = f'{emotion} - {round(proba, 3)}'\n",
    "        cv2.putText(frame, text, org, font, fontScale, color, thickness)\n",
    "        #frame = Image.fromarray(frame[..., ::-1])# for image\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515ab1a-5edb-48a3-9827-8ab3262baf64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
